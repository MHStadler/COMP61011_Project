% -----------------------------------------------------------------------
% File: references.bib
% -----------------------------------------------------------------------

@TECHREPORT{Brown04diversityin,
    author = {Gavin Brown},
    title = {Diversity in Neural Network Ensembles},
    institution = {},
    year = {2004}
}

@book{Breiman1984,
  author = {Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  publisher = {Wadsworth and Brooks},
  title = {Classification and Regression Trees},
  year = 1984
}

@Article{lucas2013,
	AUTHOR = {Lucas, D. D. and Klein, R. and Tannahill, J. and Ivanova, D. and Brandon, S. and Domyancic, D. and Zhang, Y.},
	TITLE = {Failure analysis of parameter-induced simulation crashes in  climate models},
	JOURNAL = {Geoscientific Model Development},
	VOLUME = {6},
	YEAR = {2013},
	NUMBER = {4},
	PAGES = {1157--1171}
}

@inproceedings{Dembczynski2011,
 author = {Dembczy\'{n}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H\"{u}llermeier, Eyke},
 title = {An Exact Algorithm for F-measure Maximization},
 booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
 series = {NIPS'11},
 year = {2011},
 pages = {1404--1412},
 numpages = {9}
} 

@inproceedings{Dembczynski2013,
 author = {Dembczy\'{n}ski, Krzysztof and Jachnik, Arkadiusz and Kot\l{}owski, Wojciech and Waegeman, Willem and H\"{u}llermeier, Eyke},
 title = {Optimizing the F-measure in Multi-label Classification: Plug-in Rule Approach Versus Structured Loss Minimization},
 booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
 series = {ICML'13},
 year = {2013},
 pages = {III-1130--III-1138}
}

@article{Fawcett2006,
 author = {Fawcett, Tom},
 title = {An Introduction to ROC Analysis},
 journal = {Pattern Recogn. Lett.},
 issue_date = {June 2006},
 volume = {27},
 number = {8},
 month = jun,
 year = {2006},
 issn = {0167-8655},
 pages = {861--874},
 numpages = {14}
} 

@article{Larson1931,
 author = {Larson, S. C.},
 title = {The shrinkage of the coefficient of multiple correlation.},
 journal = {Journal of Educational Psychology},
 issue_date = {1931},
 volume = {22},
 number = {1},
 year = {1931},
 pages = {45--55},
 numpages = {10}
} 

@article{Paninski2003,
 author = {Paninski, Liam},
 title = {Estimation of Entropy and Mutual Information},
 journal = {Neural Comput.},
 issue_date = {June 2003},
 volume = {15},
 number = {6},
 month = jun,
 year = {2003},
 issn = {0899-7667},
 pages = {1191--1253},
 numpages = {63}
} 


@article{Provost2003,
 author = {Provost, Foster and Domingos, Pedro},
 title = {Tree Induction for Probability-Based Ranking},
 journal = {Mach. Learn.},
 issue_date = {September 2003},
 volume = {52},
 number = {3},
 month = sep,
 year = {2003},
 issn = {0885-6125},
 pages = {199--215},
 numpages = {17}
} 

@Article{Raileanu2004,
author="Raileanu, Laura Elena
and Stoffel, Kilian",
title="Theoretical Comparison between the Gini Index and Information Gain Criteria",
journal="Annals of Mathematics and Artificial Intelligence",
year="2004",
month="May",
day="01",
volume="41",
number="1",
pages="77--93",
abstract="Knowledge Discovery in Databases (KDD) is an active and important research area with the promise for a high payoff in many business and scientific applications. One of the main tasks in KDD is classification. A particular efficient method for classification is decision tree induction. The selection of the attribute used at each node of the tree to split the data (split criterion) is crucial in order to correctly classify objects. Different split criteria were proposed in the literature (Information Gain, Gini Index, etc.). It is not obvious which of them will produce the best decision tree for a given data set. A large amount of empirical tests were conducted in order to answer this question. No conclusive results were found. In this paper we introduce a formal methodology, which allows us to compare multiple split criteria. This permits us to present fundamental insights into the decision process. Furthermore, we are able to present a formal description of how to select between split criteria for a given data set. As an illustration we apply the methodology to two widely used split criteria: Gini Index and Information Gain."
}

@inproceedings{Sawade2010,
 author = {Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},
 title = {Active Estimation of F-measures},
 booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'10},
 year = {2010},
 pages = {2083--2091},
 numpages = {9}
} 

@article{Stone1974,
 abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
 author = {M. Stone},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {111-147},
 title = {Cross-Validatory Choice and Assessment of Statistical Predictions},
 volume = {36},
 year = {1974}
}

@misc{ Uci1998,
    author = {C.L. Blake and C.J. Merz},
    title = {UCI repository of machine learning databases},
    year = {1998},
    url = {http://archive.ics.uci.edu/ml/index.php},
    note = {[Online; accessed 2017-11-01]}
  }
  
  @article{Nan2012,
  author    = {Nan Ye and
               Kian Ming Adam Chai and
               Wee Sun Lee and
               Hai Leong Chieu},
  title     = {Optimizing F-measure: {A} Tale of Two Approaches},
  journal   = {CoRR},
  volume    = {abs/1206.4625},
  year      = {2012}
}

% -----------------------------------------------------------------------
% Document End
% -----------------------------------------------------------------------
