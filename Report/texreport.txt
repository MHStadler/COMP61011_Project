% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[12pt,a4paper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage{titlesec}

\title{An Experimental Analysis of Decision Trees that split on the F-measure}
 
\author{{\large \bf Michael Stadler (MAG@Macc.Wisc.Edu)} \\
  MSc. Advanced Computer Science
  \AND {\large \bf Dave (SDJ@Macc.Wisc.Edu)} \\
  MSc. Advanced Computer Science}

\begin{document}

\maketitle


\begin{abstract}
Abstract here

\textbf{Keywords:} 
Decision Tree; F-Measure; Information Gain; Gini-Index
\end{abstract}

\section{Introduction}

The F-measure is a commonly used metric when assessing a classifier's performance \cite{Dembczynski2011}.

\section{Splitting on the F$_\beta$-measure}

This section first briefly introduces the basics of the F-measure and then goes on to illustrate how we used it as split-criteria in a decision tree.

\subsection{The F$_\beta$-measure}
In binary classifications there are four possible results when assessing a classification result. Examples of class 1 are called true positives ($TP$) if classified correctly, or false positives ($FP$) otherwise. Same goes for class 0, but in this case we speak of true negatives ($TN$) and false negatives ($FN$) \cite{Fawcett2006}.

The resulting confusion matrix is the basis for many performance metrics, such as the F$_\beta$-measure, formally written as \cite{Nan2012}:
\begin{equation}F_\beta = \frac{(1 + \beta^2) \sum_{i}s_iy_i}{\beta^2 \sum_{i}y_i + \sum_{i}s_i}\end{equation}

With the formal definitions for precision ($p$) and recall ($r$) \cite{Nan2012}:

\begin{displaymath}p = \frac{\sum_{i}s_iy_i}{\sum_{i}s_i} r = \frac{\sum_{i}s_iy_i}{\sum_{i}y_i}\end{displaymath}

it follows that the F$_\beta$-measure is the weighted harmonic mean of the two

\begin{equation}F_\beta =\frac{(1 + \beta^2) p r}{\beta^2 p + r} \Leftrightarrow   \frac{(1 + \beta^2)}{\frac{\beta^2}{r} + \frac{1}{p}}\end{equation} 

Informally, precision is the ratio of $TP$ to the number of examples classified as positive ($Y$), and recall the ratio of TP to the overall number of positives ($P$): \begin{math}p = \frac{TP}{Y} \Leftrightarrow \frac{TP}{TP + FP}\end{math} and \begin{math}r = \frac{TP}{P} \Leftrightarrow \frac{TP}{TP + FN}\end{math}. This allows us to express the F$_\beta$-measure based on TP, FP and FN:

\begin{equation}F_\beta =\frac{(1 + \beta^2)}{\frac{\beta^2 (TP+FN)}{TP} + \frac{TP+FP}{TP}} \Leftrightarrow \frac{(1 + \beta^2)TP}{(1 + \beta^2)TP + \beta^2FN + FP} \end{equation} 

We see, that a classifier with a high F$_\beta$-measure correctly classifies a lot of class 1 examples correctly, while making few classification errors. The $\beta$ factor gives weight to how much the classifier gets punished for misclassifying class 1 examples, though in praxis it is mostly just left at its default 1, also called the F$_1$-score or just F-measure \cite{Nan2012}.

\subsection{The F$_\beta$-measure as split-criteria}

The F-measure as a learning criteria has successfully been employed before, in varying ways for different models \cite{Dembczynski2013,Sawade2010}. For this analysis we use the F-measure as split criteria in a decision tree.

Our first approach was to maximize the overall F-measure after a split. Both sides of a split were first assigned a label, if the count of class 1 examples on one side, outweighed the class 0 count, we considered it to predict class 1, otherwise class 0.

If a split is considered to be positive then all of its positive examples are $TP$ and all of its negatives are $FP$. Otherwise all of its positives are $FN$. After adding the values up for both sides of the split, we can calculate the F-measure for this split. The split with the highest value is then used.

\begin{algorithm}
\caption{Maximum F-measure across a split}
p and n, the number of positive and negative examples on the left side of the split; !p and !n, the number of positive and negative examples on the other side;
\begin{algorithmic}[1]
\Function{F-Measure}{$p, n, !p, !n$}

\State $TP, FP, FN \gets 0$

\If{$p > n$}
\State $TP \gets TP + p$
\State $FP \gets FP + n$
\Else
\State $FN \gets FN + p$
\EndIf

\If{$!p > !n$}
\State $TP \gets TP + !p$
\State $FP \gets FP + !n$
\Else
\State $FN \gets FN + !p$
\EndIf

\State $fScore \gets fScore(TP, FP, FN)$

\textbf{return} $fScore$
\EndFunction
\end{algorithmic}
\end{algorithm}

This heavily emphasizes examples of class 1; a split that would lead to a high purity of class 0 on one side, is not recognized as valuable if it does not also yield a high number of true positives. This becomes a problem when class 1 and class 0 examples become harder to differentiate as the number of examples per leaf shrinks.

We developed a second approach, modeled after the Information Gain. For each side of the split we first find the majority class. That class is then considered the positive, regardless of its actual class. TP and FP are thus equal to the number of examples in the majority and minority class respectively, with FN being the number of examples for the majority class that are still on the other side of the split.

\begin{algorithm}
\caption{Weighted fScore for side}
\hspace*{\algorithmicindent} \textbf{Inputs:} p and n, the number of positive and negative examples on this side of the split; !p and !n,same for the other side of the split;
\begin{algorithmic}[1]
\Procedure{F-Measure}{$p, n, !p, !n$}

\State $count \gets p+n$
\State $wholeCount \gets count + !p + !n$

\If{$p > n$}
\State $TP \gets p$
\State $FP \gets n$
\State $FN \gets !p$
\Else
\State $TP \gets n$
\State $FP \gets p$
\State $FN \gets !n$
\EndIf

\State $fScore \gets fScore(TP, FP, FN)$

\textbf{return} $(count/wholeCount) * fScore$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The new metric is the sum of the weighted F-score of both sides, and the split with the highest delta to the pre-split F-measure is used. 

After some preliminary testing we found the second algorithm to outperform the first. For brevity's sake we will therefore focus on the second one going forward.

\section{Splitting on Impurity Metrics}

In order to assess the performance of tress that split on the F-measure, we additionally carried out our experiments with more commonly used split-criteria, Information Gain and Gini Index.

\subsection{Information Gain}

Information Gain (IG) is a measure of difference between the entropy the data prior to splitting, and the combined entropy of the two sides after the split. A collection of data is given an entropy $I_H$ according to the equation:
\begin{displaymath}I_H = \sum_{j}p_jlogp_j\end{displaymath}

Where $p_j$ is the probability an example having the label $j$. The Information Gain $IG$ of a split is calculated by deducting the weighted sum of the entropies on each side, from the entropy before the split:
\begin{displaymath} IG(D_p,f) = I_H(D_p)-\frac{n_l}{n_p}I_H(D_l)-\frac{n_r}{n_p}I_H(D_r)\end{displaymath}

\subsection{Gini Index}
The Gini Index works similar to Information Gain. Each side of the split data is given a Gini Index $I_G$ according to the equation:
\begin{displaymath}I_H = 1-\sum_{j}p_j^2\end{displaymath}

Just like with IG, the weighted sum of the impurities is then deducted from the pre-split Gini Index. 

Gini Index calculations are slightly faster to compute as they do not require a logarithm and will always yield a result in the range 0-1, regardless of the amount of classes.

A more detailed discussion of the differences between Information Gain and Gini Index is described by \citeA{Raileanu2004}.

\section{Experiments}

In this section we want to show the results we got from testing out the three different split criteria, before analyzing these results.

\subsection{Datasets}

For this paper we used three different datasets found in the UCI machine learning repository \cite{Uci1998}:
\begin{itemize}
\item Breast: Breast Cancer Wisconsin (Diagnostic): 569 examples with 30 features, and 212 class 1 examples
\item Heart: Statlog (Heart): 270 examples with 13 features, and 120 class 1 examples
\item PopFailure: Climate Model Simulation Crashes: 540 examples with 18 features, and 46 class 1 examples
\end{itemize}

For each dataset we first randomized the data then split it into five folds, four training sets, to be used in a 4-fold cross validation, and a fifth independent test set. We chose to leave out one fold from the k-fold cross validation to ensure a higher chance of an unbiased view of the performance of the model, as introduced by \citeA{Larson1931} and referenced by \citeA{Stone1974}.

From the models trained in the cross validation we were able to get the average error rate ($\bar{e}$), F-score ($\bar{f}$), and the accompanying standard deviations.

\begin{table}[!ht]
\begin{center} 
\caption{F-Measure Training Results} 
\label{sample-table} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c | c }
\hline
Set & $\bar{e}$ & $\sigma$ & $\bar{f}$ & $\sigma$ \\
\hline
Breast & .0951 & .0254 & .8669 & .0401\\
Heart & .2917 & .0532 & .6367 & .0838\\
PopFailure &  .0949 & .0175 & .4332 & .2276\\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{Gini Index Training Results} 
\label{sample-table} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c | c }
\hline
Set & $\bar{e}$ & $\sigma$ & $\bar{f}$ & $\sigma$ \\
\hline
Breast & .0885 & .0125 & .8778 & .0272\\
Heart & .1759 & .0535 & .8058 & .0671\\
PopFailure & .0787 & .4040 & .5816 & .0684\\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{Mutual Information Training Results} 
\label{sample-table} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c | c }
\hline
Set & $\bar{e}$ & $\sigma$ & $\bar{f}$ & $\sigma$ \\
\hline
Breast & .0774	& .0111 & .8963 & .0212\\
Heart & .1806 & .0532 & .7912 & .0730\\
PopFailure & .0833 & .0227 & .5066 & .0827\\
\hline
\end{tabular} 
\end{center} 
\end{table}

Per dataset and split criteria, we then chose the model with the lowest error rate as the best performing one to be tested with the test set. 

\subsection{Test Results}

Additionally to the error rate ($e$) and F-score ($f$) we used receiver operating characteristics (ROC) techniques, namely ROC graphs and curves, to investigate the performance of the trained models on the test set \cite{Fawcett2006}:

\begin{itemize}
\item A ROC graphs scatter plots the TP rate and FP rate of different classifiers, and allows for great comparison.
\item A ROC curve plots one classifiers skill in correctly discerning class 1 from class 0 examples
\item AUC is the area under the ROC curve, assigning it a numerical value
\end{itemize}

\begin{table}[!ht]
\begin{center} 
\caption{F-Measure Test Results} 
\label{sample-table} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c }
\hline
Set & $e$ & $f$ & AUC \\
\hline
Breast & .0940	& .8406 & .8485 \\
Heart & .2963 & .6190 & .7166 \\
PopFailure & .0741 & .5000 & .9701 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{Gini Index Test Results} 
\label{sample-table} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c }
\hline
Set & $e$ & $f$ & AUC \\
\hline
Breast & .0940 & .8571 & .9800 \\
Heart & .2407 & .7234 & .8186 \\
PopFailure & .0926 & .4444 & .7801 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{Mutual Information Test Results} 
\label{sample-table} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c }
\hline
Set & $e$ & $f$ & AUC \\
\hline
Breast & .0342 & .9487 & .9904 \\
Heart & .2407 & .6829 & .8814 \\
PopFailure & .0833 & .4706 & .7801 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

Each model yields one point in ROC space, with $x$ being the FP rate and $y$ the TP rate.

\begin{figure}[!ht]
\caption{Breast ROC Graph}
\centering
\includegraphics[width=0.45\textwidth]{breastROCSpace}
\end{figure}

\begin{figure}[!ht]
\caption{Heart ROC Graph}
\centering
\includegraphics[width=0.45\textwidth]{heartROCSpace}
\end{figure}

\begin{figure}[!ht]
\caption{PopFailure ROC Graph}
\centering
\includegraphics[width=0.45\textwidth]{popFailureROCSpace}
\end{figure}

To build the ROC curve of each model we used the algorithm introduced by \citeA{Fawcett2006}. Scoring values for the ROC curve are the deciding leaf's, Laplace corrected \cite{Provost2003}, class proportions.

\begin{figure}[!ht]
\caption{Breast ROC Curve}
\centering
\includegraphics[width=0.45\textwidth]{breastROCCurves}
\end{figure}

\begin{figure}[!ht]
\caption{Heart ROC Curve}
\centering
\includegraphics[width=0.45\textwidth]{heartROCCurves}
\end{figure}

\begin{figure}[!ht]
\caption{PopFailure ROC Curve}
\centering
\includegraphics[width=0.45\textwidth]{popfailureROCCurves}
\end{figure}

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{CogSci_Template}


\end{document}
