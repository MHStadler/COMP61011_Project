% 
% MSc COMP61011 Coursework Project
% LaTeX Paper -- Proceedings Format
% 

% Original : Michael Stadler (ashwin@cc.gatech.edu)       29/10/2017
% Modified : David Corlett (david.corlett-2@postgrad.manchester.ac.uk)      02/11/2017

\documentclass[12pt,a4paper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{commath}
\graphicspath{ {images/} }
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\setlength{\footskip}{2cm}
\pagenumbering{arabic}

\title{An Experimental Analysis of Decision Trees that split on the F-measure}
 
\author{{\large \bf Michael Stadler (michael.stadler@postgrad.manchester.ac.uk)} \\
  MSc. Advanced Computer Science
  \AND {\large \bf David Corlett (david.corlett-2@postgrad.manchester.ac.uk)} \\
  MSc. Advanced Computer Science}

\begin{document}

\maketitle


\begin{abstract}
The F-measure is a commonly used performance metric. In this experimental analysis we use it as a split-criteria for a decision tree. We first introduce the two established methods of Information Gain and Gini Index that serve as reference points for our classifier's performance. Then we move on to the F-measure and how we used it as a split-criteria. Each classifier is tested on three different datasets in a 4-fold cross validation, with a fifth fold left out as an independent test set. The performance of each is assessed on the average error rate and F-measure in the cross fold validation, as well as the error rate, F-Measure, ROC graph and ROC curve for the test set. Based on these metrics we analyze the classifiers, assess our method's usefulness, and show that F-measure splitting looks very promising for heavily biased datasets.

\textbf{Keywords:} 
Decision Tree; F-Measure; Information Gain; Gini-Index
\end{abstract}

\section{Introduction}

\citeA{Dembczynski2013} notes that a high F-measure, a commonly used performance metric \cite{Dembczynski2011}, means that a classifier performs well on all of its classes. For this experiment we hope exploit this by using the F-measure as a decision tree split criteria.

We first summarize the more established criteria, Information Gain and Gini Index, that serve as reference points for our classifier's performance, followed by a more in-depth introduction to the F-measure, and how we used it in our decision tree.

We then showcase the different datasets, and their properties, used in this experiment, as well as which metrics we collected from our different models.

The report concludes with an analysis of the collected data, and an assessment of the usefulness of our classifier.

\section{Splitting on Impurity Metrics}

In order to assess the performance of tress that split on the F-measure, we additionally carried out our experiments with more commonly used split-criteria, Information Gain and Gini Index.

\subsection{Information Gain}

Information Gain, or Mutual Information, is a measure of difference between the entropy the data prior to splitting, and the combined entropy of the two sides after the split \cite{Paninski2003}. The entropy ($H$) for a set of data ($D$) is defined as \cite{Paninski2003}:
\setcounter{equation}{0}
\begin{equation}H(D) = -\sum_{j}p_jlogp_j\end{equation}

Where $p_j$ is the probability an example having the label $j$. The Information Gain ($I$) of splitting a set $D_p$ via a function $f$ is calculated by deducting the weighted sum of the entropies on each side from the entropy before the split \cite{Paninski2003}:
\begin{equation} I(D_p,f) = H(D_p)-(\frac{n_l}{n_p}H(D_l)+\frac{n_r}{n_p}H(D_r))\end{equation}

\subsection{Gini Index}
Here, each side of the split ($D$) is assigned a Gini Index $G$ \cite{Breiman1984}:
\begin{displaymath}G(D_x) = 1-\sum_{j}p_j^2\end{displaymath}

Like with Information Gain, the weighted sum of the indices is deducted from the pre-split value. We recommend \citeA{Raileanu2004} for a more detailed description of the differences between Information Gain and Gini Index.

\section{Splitting on the F$_\beta$-measure}

This section first briefly introduces the basics of the F-measure and then goes on to illustrate how we used it as split-criteria in a decision tree.

\subsection{The F$_\beta$-measure}
In binary classifications there are four possible results when assessing a classification result. Examples of class 1 are called true positives ($TP$) if classified correctly, or false positives ($FP$) otherwise. Same goes for class 0, but in this case we speak of true negatives ($TN$) and false negatives ($FN$) \cite{Fawcett2006}.

\setcounter{equation}{0}
The resulting confusion matrix is the basis for many performance metrics, such as the F$_\beta$-measure, formally written as \cite{Nan2012}:
\begin{equation}F_\beta = \frac{(1 + \beta^2) \sum_{i}s_iy_i}{\beta^2 \sum_{i}y_i + \sum_{i}s_i}\end{equation}

With the formal definitions for precision ($p$) and recall ($r$) \cite{Nan2012}:

\begin{displaymath}p = \frac{\sum_{i}s_iy_i}{\sum_{i}s_i} r = \frac{\sum_{i}s_iy_i}{\sum_{i}y_i}\end{displaymath}

it follows that the F$_\beta$-measure is the weighted harmonic mean of the two

\begin{equation}F_\beta =\frac{(1 + \beta^2) p r}{\beta^2 p + r} \Leftrightarrow   \frac{(1 + \beta^2)}{\frac{\beta^2}{r} + \frac{1}{p}}\end{equation} 

Informally, precision is the ratio of $TP$ to the number of examples classified as positive ($Y$), and recall the ratio of TP to the overall number of positives ($P$): \begin{math}p = \frac{TP}{Y} \Leftrightarrow \frac{TP}{TP + FP}\end{math} and \begin{math}r = \frac{TP}{P} \Leftrightarrow \frac{TP}{TP + FN}\end{math}. This allows us to express the F$_\beta$-measure based on TP, FP and FN:

\begin{equation}F_\beta =\frac{(1 + \beta^2)}{\frac{\beta^2 (TP+FN)}{TP} + \frac{TP+FP}{TP}} \Leftrightarrow \frac{(1 + \beta^2)TP}{(1 + \beta^2)TP + \beta^2FN + FP} \end{equation} 

We see, that a classifier with a high F$_\beta$-measure correctly classifies a lot of class 1 examples correctly, while making few classification errors. The $\beta$ factor gives weight to how much the classifier gets punished for misclassifying class 1 examples, though in praxis it is mostly just left at its default 1, also called the F$_1$-score or just F-measure \cite{Nan2012}.

\subsection{The F-measure as split-criteria}

The F-measure as a learning criteria has successfully been employed before, in varying ways for different models \cite{Dembczynski2013,Sawade2010}. Here we use the F-measure as split criteria in a decision tree, modeled after the Information Gain.

To split on the F-measure we first find the majority class for each side of the split. That class is then considered the positive, regardless of its actual class. TP and FP are thus equal to the number of examples in the majority ($S_p$) and minority class ($S_n$) respectively, with FN being the number of examples of the majority class on the other side ($\neg S_p$).

\setcounter{equation}{0}
\begin{equation}F(S) = \frac{2 S_p}{(2S_p)+\neg S_p+S_n}\end{equation}

The new metric is the sum of the weighted F-measure of each side, and the split with the highest delta to the pre-split F-measure is then used going forward. 

\begin{equation}
\Delta F = \abs{F(S) - (\frac{n_l}{n} F(S_l) + \frac{n_r}{n} F(S_r))} 
\end{equation}

High deltas yield balanced purities on both sides of the split, preventing the tree from over fitting on majority classes.

\section{Experiments}

In this section we want to show the results we got from testing out the three different split criteria, before analyzing these results.

\subsection{Datasets}

For this paper we used three proved \cite{Brown04diversityin,lucas2013} datasets from the UCI machine learning repository \cite{Uci1998}:
\begin{itemize}
\item Breast: Breast Cancer Wisconsin (Diagnostic): 569 examples with 30 features, and 212 class 1 examples
\item Heart: Statlog (Heart): 270 examples with 13 features, and 120 class 1 examples
\item PopFailure: Climate Model Simulation Crashes: 540 examples with 18 features, and 46 class 1 examples
\end{itemize}

Maximum depth and minimum element count per leaf indicate when the tree stops splitting. We left the maximum depth at its default of $10$ and defined a minimum element count for each set (based on preliminary testing): Heart $10$, Breast $20$, PopFailure $5$. We did not fine tune the parameters beyond that and each classifier was trained with the same constraints.

For each dataset we first randomized the data then split it into five folds, four training sets, to be used in a 4-fold cross validation, and a fifth independent test set. We chose to leave out one fold from the k-fold cross validation to ensure a higher chance of an unbiased view of the performance of the model, as introduced by \citeA{Larson1931} and referenced by \citeA{Stone1974}.

For each set, we trained a decision tree using our F-measure approach ($F$), Gini Index ($G$), and Mutual Information ($M$) in cross validation. From these models we were able to get the average error rate ($\bar{e}$), F-score ($\bar{f}$), and the accompanying standard deviations.

\begin{table}[!ht]
\begin{center} 
\caption{Breast Training Results} 
\label{table:breastTr} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c | c }
\hline
Type & $\bar{e}$ & $\sigma$ & $\bar{f}$ & $\sigma$ \\
\hline
F & .0951 & .0254 & .8669 & .0401 \\
G & .0885 & .0125 & .8778 & .0272 \\
M & .0774 & .0111 & .8963 & .0212 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{Heart Training Results} 
\label{table:heartTr} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c | c }
\hline
Type & $\bar{e}$ & $\sigma$ & $\bar{f}$ & $\sigma$ \\
\hline
F & .2917 & .0532 & .6367 & .0838 \\
G & .1759 & .0535 & .8058 & .0671 \\
M & .1806 & .0532 & .7912 & .0730 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{PopFailure Training Results} 
\label{table:popTr} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c | c }
\hline
Type & $\bar{e}$ & $\sigma$ & $\bar{f}$ & $\sigma$ \\
\hline
F & .0949 & .0175 & .4332 & .2276 \\
G & .0787 & .0404 & .5816 & .0684 \\
M & .0833 & .0227 & .5066 & .0827 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

Per dataset and split criteria, we then chose the model with the lowest error rate as the best performing one to be tested with the test set. 

\subsection{Test Results}

Additionally to the error rate ($e$) and F-score ($f$) we used receiver operating characteristics (ROC) techniques, namely ROC graphs and curves, to investigate the performance of the trained models on the test set \cite{Fawcett2006}:

\begin{itemize}
\item A ROC graphs scatter plots the TP rate and FP rate of different classifiers, and allows for great comparison.
\item A ROC curve plots one classifiers skill in correctly discerning class 1 from class 0 examples
\item AUC is the area under the ROC curve, assigning it a numerical value
\end{itemize}

\begin{table}[!ht]
\begin{center} 
\caption{Breast Test Results} 
\label{table:breastTe} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c }
\hline
Type & $e$ & $f$ & AUC \\
\hline
F & .0940 & .8406 & .8485 \\
G & .0940 & .8571 & .9800 \\
M & .0342 & .9487 & .9904 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{Heart Test Results} 
\label{table:heartTe} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c }
\hline
Type & $e$ & $f$ & AUC \\
\hline
F & .2963 & .6190 & .7166 \\
G & .2407 & .7234 & .8186 \\
M & .2407 & .6829 & .8814 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

\begin{table}[!ht]
\begin{center} 
\caption{PopFailure Test Results} 
\label{table:popTe} 
\vskip 0.12in
\begin{tabular}{ c | c | c | c }
\hline
Type & $e$ & $f$ & AUC \\
\hline
F & .0741 & .5000 & .9701 \\
G & .0926 & .4444 & .7801 \\
M & .0833 & .4706 & .7801 \\
\hline
\end{tabular} 
\end{center} 
\end{table}

Each model yields one point in ROC space, with $x$ being the FP rate and $y$ the TP rate.

\begin{figure}[!ht]
\caption{Breast ROC Graph}
\label{fig:BreastRocG} 
\centering
\includegraphics[width=0.45\textwidth]{breastROCSpace}
\end{figure}

\begin{figure}[!ht]
\caption{Heart ROC Graph}
\label{fig:HeartRocG} 
\centering
\includegraphics[width=0.45\textwidth]{heartROCSpace}
\end{figure}

\begin{figure}[!ht]
\caption{PopFailure ROC Graph}
\label{fig:PopRocG} 
\centering
\includegraphics[width=0.45\textwidth]{popFailureROCSpace}
\end{figure}

To build the ROC curve of each model we used the algorithm introduced by \citeA{Fawcett2006}. Scoring values for the ROC curve are the deciding leaf's, Laplace corrected \cite{Provost2003}, class proportions.

\begin{figure}[!ht]
\caption{Breast ROC Curves}
\label{fig:BreastRocC} 
\centering
\includegraphics[width=0.45\textwidth]{breastROCCurves}
\end{figure}

\begin{figure}[!ht]
\caption{Heart ROC Curves}
\label{fig:HeartRocC} 
\centering
\includegraphics[width=0.45\textwidth]{heartROCCurves}
\end{figure}

\begin{figure}[!ht]
\caption{PopFailure ROC Curves}
\label{fig:PopRocC} 
\centering
\includegraphics[width=0.45\textwidth]{popfailureROCCurves}
\end{figure}

\section{Analysis}

The first point of interest are the training results. Looking at the results for the Breast set (Table \ref{table:breastTr}) we see low error values across the board, with a small deviation in error rates. The F-measures also indicate good performance, but with a higher variation. This can be explained due to varying amount of class 1 examples available in the validation set of the data. If the error rate is somewhat constant, the F-Measure is lower if there aren't a lot of examples to classify as $TP$. Looking at the test results for breast (Table \ref{table:breastTe}) we can see that the values here are pretty much in tune with what we see in the training results. Our F-measure and the Gini Index perform almost exactly as they do on the training data, only $M$ is far off, in this case vastly outperforming on the test data.

Overall we can see that the models are all rather well fitted. We get a steady training error, which is also in-line with the test error. Mutual Information seems to have well fit on the training results and simply gotten a good draw on the test set.

This is also represented in the ROC Graph (Figure \ref{fig:BreastRocG}), as we see $M's$ great performance placed in the top far left \cite{Fawcett2006}. $F$ and $G$ are about even, with the F-measure taking a more conservative stance to bottom left, as opposed to the Gini Index's liberal top right \cite{Fawcett2006}. Going back to the numbers we see that the Gini Index outperforms the F-measure by a bit. 

Overall, not too bad of a result for our experimental measure: It falls behind the established ones but not considerably so.

The data for the Heart dataset tells a different story (Table \ref{table:heartTr}). Here we get a rather high error on all criteria with an almost even deviation. Same for the F-measure. The relative high error suggest that we have an ill fitted model, though it is hard to tell whether it is over or under fitted. The test error is barely not within the standard deviation, so not a clear indicator either.In the ROC Graph {Figure \ref{fig:HeartRocG} we again see $G$ with a liberal stance, and $M$ and $F$ being more conservative, with a slight edge for $M$. 

Comparing the ROC Curves of the well fitted classifiers (\ref{fig:BreastRocC}) with the ones for the heart dataset (\ref{fig:HeartRocC}). The curves are generated by sorting the classification results descending by their estimate of the likeliness that the example is of class 1. Mistakes in the lower left half therefore can mean one of two things. Either we were very sure of a example being class 1 but got it wrong, or we don't have members with a high chance of class 1. In the breast ROC Curves we see that we have many points on, or almost on the y axis, meaning we make few mistakes when we think a class is positive. In the Heart curves on the other hand we see a lot of errors being made in the lower left. Looking at $F$ we see that we also make some mistakes further up, which suggest that the model is under fit, having poor odds for either class membership, resulting in many classification errors. We see a similar effect in $G$ and $M$, though they fair a lot better, making their mistakes further up. So we still make some mistakes in regions where we would be rather sure, but then we get a good rate once we become surer of an example's class 0 membership. If we consider $G$ and $M$ to be better fit, we can see them converging towards a similar graph displayed in Figure \ref{fig:BreastRocC}, with $G$ fitting on the on the $TP-Rate$ first, and $M$ on the $FP-Rate$.

With this in mind we would argue that it is likelier that the classifier's under fitted on the data, but no clear argument can be made with data available. Further experiments with varying depths and minimum examples counts are needed, all we can say for sure is that the models are ill fitted. While not ideal, this still gives us some insight in the behavior of our $F$ classifier, and how it behaves under bad circumstances.

This leaves the last of the three sets where the class 0 examples vastly outweigh the class 1 examples. In the Training Results (\ref{table:popTr}) we see low test errors, but also low F-Measures. This can be explained by the disproportional class distribution: with few class 1 examples to get right, it's hard to get many class 0 examples wrong. This leads to low error, but does not necessarily mean we are good at discerning classes. We only get a few of the very scarce class 1 examples right, leading to a low F-score. Compared to the Test Results (\ref{table:popTe}) we see that the error rates are in line with we had in the training stage, with room for improvement in the F-Measure department.

The ROC graph (Figure \ref{fig:PopRocG}) shows all classifiers even, mistaking few class 1 examples for class 0, but struggling to correctly classify the few class 1 examples at the same time.


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{CogSci_Template}


\end{document}
